## 概述
传统分布式深度学习任务(比如Tensorflow Job)，提交训练任务后，无法再动态调整Worker的数量。
但是在某些场景下，我们需要弹性训练的能力： 例如，集群中存在一批算力需求波动大的高优任务（比如周期性波动的在线任务), 集群的整体平均资源利用率是较低的，因为波谷期的资源没有得到充分利用。这种场景下，可以在集群中运行弹性训练任务，充分利用集群闲置资源。
MPI Operator 通过对接Horovod的Elastic模式，可以让MPI Job 具备**动态调整训练Workers数量**的能力。您可以结合云原生的**低成本算力能力（竞价实例）以及多维度弹性能力**（如HPA、VPA、CA）充分利用空闲的算力资源，以大幅降低训练成本。本文介绍如何运行MPI任务。


## 前置条件

- AI环境中已经安装了[MPI Operator]()。
- AI环境中有GPU资源。

## 操作指南
`MPI-Operator` 官方提供了多个训练案例，在本文档中我们以 Horovod Elastic 训练为例。

### 准备训练代码

本示例中我们使用Horovod官方提供的示例代码[`tensorflow2_mnist_elastic.py`](https://github.com/horovod/horovod/blob/v0.20.0/examples/elastic/tensorflow2_mnist_elastic.py)

### 制作训练镜像

Horovod 官方提供的 `horovod/horovod:0.20.0-tf2.3.0-torch1.6.0-mxnet1.5.0-py3.7-cpu` 镜像中已经包含了该训练代码。
> ! 
> - 如果采用其他 Horovod 镜像作为基础镜像来制作训练镜像，请将训练代码复制至到镜像内
- 由于训练代码是基于 TensorFlow 2.3.0 构建的，如果采用不同的 TensorFlow 版本，请注意 API 兼容性。
> 
#### 腾讯云自研优化镜像
>? 镜像仓库 `ccr.ccs.tencentyun.com/ti_containers/ti-tensorflow` 中内置了由**腾讯优图实验室和机智团队**合作开发的Ti-horovod，针对腾讯云网络环境特点进行了**定制优化**。
>
**标签（tag）列表**

* `1.15.2-gpu-cu100-py3`
* `2.0.0-gpu-cu100-py3`
* `2.1.0-gpu-cu101-py3`
* `2.4.0-gpu-cu112-py3`
* `2.4.0-gpu-cu113-py3`


### 任务提交

1. 准备一个 MPIJob 的 [yaml 文件](https://raw.githubusercontent.com/kubeflow/mpi-operator/master/examples/horovod/tensorflow-mnist-elastic.yaml)：定义一个 Horovod Elastic 任务，至少需要 1 个 Worker，至多接受 3 个 Worker，一开始的时候创建 2 个 Worker。

```yaml
apiVersion: kubeflow.org/v1
kind: MPIJob
metadata:
  name: tensorflow-mnist-elastic
spec:
  slotsPerWorker: 1
  cleanPodPolicy: Running
  mpiReplicaSpecs:
    Launcher:
      replicas: 1
      template:
        spec:
          containers:
          - image: horovod/horovod:0.20.0-tf2.3.0-torch1.6.0-mxnet1.5.0-py3.7-cpu
            name: mpi-launcher
            command:
            - horovodrun
            args:
            - -np
            - "2"
            - --min-np
            - "1"
            - --max-np
            - "3"
            - --host-discovery-script
            - /etc/mpi/discover_hosts.sh
            - python
            - /examples/elastic/tensorflow2_mnist_elastic.py
            resources:
              limits:
                cpu: 1
                memory: 2Gi
    Worker:
      replicas: 2
      template:
        spec:
          containers:
          - image: horovod/horovod:0.20.0-tf2.3.0-torch1.6.0-mxnet1.5.0-py3.7-cpu
            name: mpi-worker
            resources:
              limits:
                cpu: 2
                memory: 4Gi
```

2. 通过 `kubectl` 提交该 MPIJob：

```shell
kubectl create -f ./tensorflow-mnist-elastic.yaml
```

3. 用户如果需要修改 Worker 的数量，可以通过 `kubectl edit` 或 `kubectl patch` 来修改该 MPIJob 的 `spec.mpiReplicaSpecs[Worker].replicas`。
